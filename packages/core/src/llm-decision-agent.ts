import { nanoid } from "nanoid";
import { inject, injectable } from "tsyringe";

import {
  Agent,
  type AgentDefinition,
  type AgentMemories,
  type AgentPreloads,
  type AgentProcessInput,
  type AgentProcessOptions,
  type CreateAgentMemoriesSchema,
  type CreateAgentMemoriesType,
  type CreateAgentOptions,
  type CreateAgentPreloadsSchema,
  type CreateAgentPreloadsType,
} from "./agent";
import { TYPES } from "./constants";
import type { Context, ContextState } from "./context";
import type { TypeToSchema } from "./definitions/data-schema";
import { toRunnableMemories } from "./definitions/memory";
import type {
  BindAgentInput,
  BoundAgent,
  OmitBoundAgentInput,
} from "./definitions/preload";
import type { LLMAgentDefinition } from "./llm-agent";
import type {
  LLMModel,
  LLMModelInputMessage,
  LLMModelInputs,
} from "./llm-model";
import type { Runnable, RunnableInput, RunnableOutput } from "./runnable";
import {
  OrderedRecord,
  extractOutputsFromRunnableOutput,
  renderMessage,
} from "./utils";
import { prepareMessages } from "./utils/message-utils";
import type {
  ExtractRunnableInputTypeIntersection,
  ExtractRunnableOutputType,
} from "./utils/runnable-type";
import { outputsToJsonSchema } from "./utils/structured-output-schema";

@injectable()
export class LLMDecisionAgent<
  I extends RunnableInput = RunnableInput,
  O extends RunnableOutput = RunnableOutput,
  State extends ContextState = ContextState,
  Preloads extends AgentPreloads = AgentPreloads,
  Memories extends AgentMemories = AgentMemories,
> extends Agent<I, O, State, Preloads, Memories> {
  static create = create;

  constructor(
    @inject(TYPES.definition)
    public override definition: LLMDecisionAgentDefinition,
    @inject(TYPES.context) context?: Context<State>,
    @inject(TYPES.llmModel) public model?: LLMModel,
  ) {
    super(definition, context);

    this.model ??= context?.resolveDependency(TYPES.llmModel);
  }

  async process(
    input: AgentProcessInput<I, Preloads, Memories>,
    options: AgentProcessOptions<Preloads, Memories>,
  ) {
    const { definition, context, model } = this;
    if (!model) throw new Error("LLM model is required");
    if (!context) throw new Error("Context is required");

    const { originalMessages, messagesWithMemory } = prepareMessages(
      definition,
      input,
      options.memories,
    );

    const cases = await Promise.all(
      OrderedRecord.map(definition.cases, async (t) => {
        if (!t.runnable?.id) throw new Error("Runnable is required");

        const runnable = await context.resolve<Runnable<I, O>>(t.runnable.id);

        // TODO: auto generate name by llm model if needed
        const name = t.name || runnable.name;
        if (!name) throw new Error("Case name is required");

        return { ...t, name, runnable };
      }),
    );

    const llmInputs: LLMModelInputs = {
      messages: messagesWithMemory,
      modelOptions: definition.modelOptions,
      tools: cases.map((t) => {
        // Filter inputs that are bound from AI
        const inputsFromAI = OrderedRecord.fromArray(
          OrderedRecord.filter(
            t.runnable.definition.inputs,
            (i) => t.input?.[i.id]?.from === "ai",
          ),
        );

        const parameters =
          inputsFromAI.$indexes.length > 0
            ? outputsToJsonSchema(inputsFromAI)
            : {};

        return {
          type: "function",
          function: {
            name: t.name,
            description: t.description,
            parameters,
          },
        };
      }),
      toolChoice: "required",
    };

    const { toolCalls } = await model.run(llmInputs);

    // TODO: support run multiple calls

    const { name: functionNameToCall, arguments: args } =
      toolCalls?.[0]?.function ?? {};
    if (!functionNameToCall) throw new Error("No any runnable called");

    const caseToCall = cases.find((i) => i.name === functionNameToCall);
    if (!caseToCall) throw new Error("Case not found");

    // Prepare arguments generated by LLM model
    const llmArgs = args ? JSON.parse(args) : {};

    // TODO: check result structure and omit undefined values
    const output = await caseToCall.runnable.run(
      { ...input, ...llmArgs },
      { stream: true },
    );

    return extractOutputsFromRunnableOutput(output, ({ $text, ...json }) =>
      this.updateMemories([
        ...originalMessages,
        {
          role: "assistant",
          content: renderMessage("{{$text}}\n{{json}}", { $text, json }).trim(),
        },
      ]),
    );
  }
}

function create<
  Case extends BoundAgent,
  InputType extends ExtractRunnableInputTypeIntersection<Case["runnable"]>,
  OutputType extends ExtractRunnableOutputType<Case["runnable"]>,
  State extends ContextState,
  Preloads extends CreateAgentPreloadsSchema<TypeToSchema<InputType>>,
  Memories extends CreateAgentMemoriesSchema<
    TypeToSchema<InputType>,
    {
      /**
       * Whether this memory is primary? Primary memory will be passed as messages to LLM chat model,
       * otherwise, it will be placed in a system message.
       *
       * Only one primary memory is allowed.
       */
      primary?: boolean;
    }
  >,
>(
  options: Omit<
    CreateAgentOptions<never, never, State, Preloads, Memories>,
    "inputs" | "outputs"
  > & {
    cases: { [name: string]: Case };

    /**
     * Options for LLM chat model.
     */
    modelOptions?: LLMModelInputs["modelOptions"];

    /**
     * Messages to be passed to LLM chat model.
     */
    messages?: LLMModelInputMessage[];
  },
): LLMDecisionAgent<
  OmitBoundAgentInput<Case, "ai">,
  OutputType,
  State,
  CreateAgentPreloadsType<TypeToSchema<InputType>, Preloads>,
  CreateAgentMemoriesType<TypeToSchema<InputType>, Memories>
> {
  const agentId = options.name || nanoid();

  const cases: OrderedRecord<LLMDecisionCase> = OrderedRecord.fromArray(
    Object.entries(options.cases).map(([name, c]) => {
      const bindInputs = Object.entries(
        (c.input as { [key: string]: BindAgentInput }) ?? {},
      );

      return {
        id: nanoid(),
        name: name || c.runnable.name,
        description: c.description,
        runnable: { id: c.runnable.id },
        input: Object.fromEntries(
          bindInputs.map(([inputName, v]) => {
            const input =
              c.runnable.definition.inputs[inputName] ||
              OrderedRecord.find(
                c.runnable.definition.inputs,
                (i) => i.name === inputName,
              );
            if (!input) throw new Error(`Input ${inputName} not found`);

            return [input.id, v];
          }),
        ),
      };
    }),
  );

  const inputs = OrderedRecord.merge(
    ...Object.values(options.cases).map((i) => i.runnable.definition.inputs),
  );

  const outputs = OrderedRecord.fromArray(
    OrderedRecord.map(
      OrderedRecord.merge(
        ...Object.values(options.cases).map(
          (i) => i.runnable.definition.outputs,
        ),
      ),
      (o) => ({ ...o, required: false }),
    ),
  );

  const memories = toRunnableMemories(agentId, inputs, options.memories ?? {});
  const primaryMemoryNames = Object.entries(options.memories ?? {})
    .filter(([, i]) => i.primary)
    .map(([name]) => name);

  if (primaryMemoryNames && primaryMemoryNames.length > 1) {
    throw new Error("Only one primary memory is allowed");
  }

  const messages = OrderedRecord.fromArray(
    options.messages?.map((i) => ({
      id: nanoid(),
      role: i.role,
      content: i.content,
    })),
  );

  return new LLMDecisionAgent(
    {
      id: agentId,
      name: options.name,
      type: "llm_decision_agent",
      inputs,
      outputs,
      messages,
      primaryMemoryId: primaryMemoryNames?.at(0),
      memories,
      modelOptions: options.modelOptions,
      cases,
    },
    options.context,
  );
}

export interface LLMDecisionAgentDefinition
  extends AgentDefinition,
    Pick<LLMAgentDefinition, "modelOptions" | "messages" | "primaryMemoryId"> {
  type: "llm_decision_agent";

  cases?: OrderedRecord<LLMDecisionCase>;
}

export interface LLMDecisionCase {
  id: string;

  name?: string;

  description?: string;

  runnable?: {
    id?: string;
  };

  input?: { [inputId: string]: BindAgentInput };
}
